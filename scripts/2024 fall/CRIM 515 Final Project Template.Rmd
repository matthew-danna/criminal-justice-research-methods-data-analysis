---
title: "CRIM 515 Final Project Template"
author: "My Name"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  cleanrmd::html_document_clean:
    theme: markdown-modest
    toc: yes
    toc_float: 
      collapsed: true
---

The Final Project follows the same general flow as the Research Projects, as outlined below. Use this format as much or as little as you'd like, as long as the required components are covered.

# 1. Research Question

### INSTRUCTIONS

The goal of this project is to **forecast** future **calls for service** activity in the **City of Fairfax, Virginia** for calendar year 2025. 

These instructions will demonstrate how to forecast using an ***ARIMA model***. An ARIMA model is an Autoregressive Integrated Moving Average. Let's breakdown these terms:
  - **Autoregressive**: "auto" means self, and "regressive" refers to using prior values. This is a univariate model
  - **Integrated**: integrated refers to the process of making data stationary, 


Your research question should consider **behavior**, **space**, and **time**. You'll either *measure* or *control* for each of these. 

- Behavior: what call type(s)
- Space: where
- Time: when

### EXAMPLE
Using 2008-2024 data, forecast **trespass** calls for service counts and locations in the City of Fairfax, VA for each month in 2025.

# 2. Data

### Acquire

1. Load the libraries you need for success:
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(leaflet)
library(sf)
library(tigris)
library(zoo)
library(aTSA)
library(tseries)
library(forecast)
```

2. Get the latest Fairfax calls for service data:
```{r}
calls.full <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", "1gniFP35GEfxycgpKGHAzkgSYmjTteTKm"))
```

### Wrangle

3. Format the date, as a date, and add a month column for later:
```{r}
calls.full$DATE <- as.Date(calls.full$date, format = "%Y-%m-%d")
calls.full$MONTH <- months(calls.full$DATE)
```

4. Determine the total count and percentage of each call type:
```{r}
sum.calls <- calls.full %>%
  group_by(type) %>%
  summarise(call.count = n()) %>%
  mutate(pct = round(call.count/sum(call.count)*100,2))
```

5. Filter the call types to forecast:
```{r}
group <- c("TRESPASSING", "TRAFFIC STOP")
calls.subset <- calls.full %>% filter(calls.full$type %in% group)
```

# 3. Methods

### INSTRUCTIONS

**Building** a relevant methodology is largely dependent on your research question and data. Any good methodology involves multiple steps for ***acquiring***, ***wrangling***, ***calculating***, ***visualizing***, and ***analyzing*** data. There is no single right answer here; rather, it’s the ability to craft a unique, distinct workflow that results in innovative work. 

Some important considerations for ***how*** you use your data include:

  - **SPACE**:
      - Overall: ***Fairfax, VA***
      - Unit of Measurement: ***Neighborhoods?***
  
  - **TIME**: 
      - Overall: time range for your datas
      - Unit of Measurement: ***Days, Months, Years***
  
  - **BEHAVIOR**:
      - Overall: Types, categories
      - Unit of Measurement: ***Call type(s)***

Ultimately, your methodology will result in analysis that becomes findings. Some things to consider:

- What are the trends? 
- What happens the most, what happens the least?
- Are there anomalies? 
- What do the statistics tell you? 

### Calculate

1. Calculate calls per day:
```{r}
calls <- calls.subset %>%
  group_by(DATE) %>%
  summarise(CALL.COUNT = n())
```

2. Fill in blank days:
```{r}
calls <- calls %>%
  complete(DATE = seq.Date(min(DATE), max(DATE), by = "day")) %>%
  mutate(WEEKDAY = lubridate::wday(DATE, label = T, week_start = 1), 
         MONTH = lubridate::month(DATE, label = T, abbr = F),
         WEEK = isoweek(DATE),
         DAY = day(DATE),
         YEAR = year(DATE))
# replace the NAs with 0s
calls <- replace(calls, is.na(calls), 0)
```

3. Change the data type, update the sequencing:
```{r}
cleaned.calls <- zoo(calls$CALL.COUNT, 
                       seq(from = as.Date(min(calls$DATE)), 
                           to = as.Date(max(calls$DATE)), by = 1))
```

4. Generate basic summary stats and a basic graph:
```{r}
summary(cleaned.calls)
plot(cleaned.calls)
title("My Calls Per Day") # this adds a title to your graph
```

5. Make the data stationary (normalized, as a deviation from the mean), and a new basic graph:
```{r}
stationary.calls <- diff(cleaned.calls)
plot(stationary.calls)
```

6. Conduct an [Augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) for stationariness:
```{r}
adf.test(as.matrix(stationary.calls)) 
```

7. Build an ARIMA function
```{r}
arima.calls <- auto.arima(stationary.calls)
arima.calls
```

8. Use the ARIMA function to do a sample forecast for the next week of data:
```{r}
# h = the number of units of time to measure
forecast.7days <- forecast(arima.calls, h=7)
additional.7days <- round(sum(forecast.7days$upper[,2]),0)
additional.7days
```

9. Identify the number of days between the last date in your dataset and the end of 2025:
```{r}
forecast.window <- as.numeric(as.Date("2025-12-31")-max(calls$DATE))
```

10. Forecast the number of calls per day for everyday in that window:
```{r}
forecast.2025 <- forecast(arima.calls, h=forecast.window)
```

11. Extract the forecasted values as a table, clean up the column names, add the forecast date, and month:
```{r}
forecast.values <- as.data.frame(forecast.2025$upper)
colnames(forecast.values) <- c("CI80", "CI90")
forecast.values$ID <- seq.int(nrow(forecast.values))
forecast.values$DATE <- as.Date(max(calls$DATE) + forecast.values$ID)
forecast.values$MONTH <- months(forecast.values$DATE)
```

12. Filter to 2025, summarize forecasts by month:
```{r}
forecast.values.2025 <- subset(forecast.values, forecast.values$DATE > '2024-12-31')
forecast.months <- forecast.values.2025 %>%
  group_by(MONTH) %>%
  summarise(FORECAST.90 = round(sum(CI90),0), FORECAST.80 = round(sum(CI80),0))
forecast.months$DIFF <- forecast.months$FORECAST.90 - forecast.months$FORECAST.80
```


### Visualize

1. Graph of calls per day with a trend line:
```{r}
graph.calls <- ggplot(calls, aes(x=DATE, y=CALL.COUNT)) + 
  geom_line() + 
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") + 
  xlab("Years") + 
  ylab("Call Count") + 
  ggtitle("TITLE HERE") + 
  geom_area(fill="lightblue", color="black")

graph.calls + 
  geom_smooth(method = lm, col = "red", se = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

2. Tweak the data table for making a graph:
```{r}
forecast.months$MONTH <- factor(forecast.months$MONTH, levels = forecast.months$MONTH)
forecast.long <- pivot_longer(forecast.months, cols = c(FORECAST.80, DIFF), 
                          names_to = "Category", values_to = "Value")
forecast.long$Category <- gsub("DIFF", "90% Confidence Interval", forecast.long$Category)
forecast.long$Category <- gsub("FORECAST.80", "80% Confidence Interval", forecast.long$Category)
```

3. Graph your monthly forecasts:
```{r}
ggplot(forecast.long, aes(x = MONTH, y = Value, fill = fct_rev(Category))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = FORECAST.90), size = 3, colour = 'white', position = position_stack(vjust = 0.5)) + 
  labs(title = "City of Fairfax 2025 Monthly Forecast", 
       x = "Month", 
       y = "Call Count") +
  scale_fill_manual(values = c("90% Confidence Interval" = "blue",
                               "80% Confidence Interval" = "grey"),
                    name = "Forecasts") +
  scale_x_discrete(limits = c("January", "February", "March", "April", "May", "June", "July", "August",
                              "September", "October", "November", "December")) + 
  coord_cartesian(ylim = c(300, 700)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

3. Map your monthly hotspots.

  - Get the city outline and roads
```{r, warning=FALSE, message=FALSE, results='hide'}
fairfax.roads <- roads("VA", "Fairfax city")
fairfax.outline <- county_subdivisions("VA", "Fairfax city")
```
  - Filter your data by month
  - Choose a hotspot method
  - Facet by year
```{r, warning=FALSE}
ggplot() +
  geom_sf(data = fairfax.outline, color = "grey") +
  geom_hex(aes(x = lon, y = lat), data = subset(calls.subset, calls.subset$MONTH == 'January'), bins = 15) +
  scale_fill_continuous(type = "viridis") +
  geom_sf(data = fairfax.roads, color = "black", alpha = 0.4) +
  theme_classic() +
  facet_wrap(~ year)
```

  - Repeat for each month.

# 4. Findings

### INSTRUCTIONS

So, that workflow and methodology you created? Use it to answer your research question. Based on the data acquired and the methods applied, what is your answer to the question? How confident are you in the results? What else? Is anything missing? Is the data complete? Is it accurate? Make sure to caveat your findings as necessary. Related, this section should be written in clear, concise, plain English statements. The methodology is for complex thought and processes; the findings are for easily digestible content.

In the context of operating in a real criminal justice organization, how do your findings ***inform*** decision-making processes? Does what you’ve discovered ***confirm/deny*** conventional wisdom? How does a criminal justice organization ***operationalize*** these findings? Why do your findings matter, and what does a successful ***integration*** of your work look like? Be bold, and be specific. For example, check out [this](https://www.start.umd.edu/pubs/UIC_MPAC_START_SafeSpaces_CommunityBasedOrganizations_June2019.pdf), [this](https://start.umd.edu/pubs/UIC_MPAC_START_SafeSpaces_PublicHealthFramework_June2019.pdf), and [this](https://cebcp.org/evidence-based-policing/what-works-in-policing/research-evidence-review/focused-deterrence/). Sure, they are violent extremism, but the way they were built can be really useful in this context, too.  

# Writing Tips

Related, some general notes about writing. Welcome to grad school, where the quality of your writing is held to a higher standard. Write directly. Thus, avoid using these words and phrases:

* Simply. Very. Indeed. Specifically. Perhaps. Arguably. Usually. Essentially. Could possibly. Could potentially. Necessarily. Only. Especially. In order to. Likely. Primarily. Generally. It is important that. It is essential to note.

Also, words have meanings. [Some are stronger than others](https://www.themuse.com/advice/185-powerful-verbs-that-will-make-your-resume-awesome). [Others aren’t real](https://www.rd.com/article/words-that-arent-words/).

Further, here’s some other strategies to consider:

* Read sentences aloud… but don’t write like you speak.
* Ask yourself - does this sentence make sense, by itself.
* It’s ok to occasionally use quotes, don’t copy specific language or words.
* If you don’t know the meaning of a word, don’t use it!

And finally, [this is just fun](https://www.thisworddoesnotexist.com/).

# Other Information/Reminders

1. When submitting your projects, you are submitting either an **HTML** or **PDF** output of your Markdown file, as an upload to Canvas - not via email.
2. When you are ready to see what the output looks like, **click the 'Knit' button.** The output file will be saved to the same folder this script is saved to.
3. *Never ever ever include any install.packages functions.* Either comment them out with a #, or delete them.

# Grading: 30pts

#### 1. Research Question
***1pt***

Provide a brief paragraph on the research question. State the question, in terms of behavior, space, and time. Define key terms and concepts, to include how variables will be measured.

#### 3. Data 
***1pt***

Provide a brief paragraph on the data used in this study. This includes the specific source(s), and the specific temporal and spatial constraints. Address any major advantages and disadvantages with using these data compared to other sources. Be specific enough that a reader could replicate this work.

#### 4. Methods
***2pts***

Provide a brief paragraph on the research methods leveraged to answer this question. This includes the software, calculations, skills, techniques, and unique workflows used to analyze your data and develop an answer. You do NOT need to describe click-by-click instructions or lines of code describing how you did things; you DO need to describe a logical process that is specific enough that a reader could replicate. 

#### 5. Findings
***12pts***

Provide one brief paragraph (4-5 sentences) per month forecasted (*2pts each*). Each paragraph should ___Your findings should be descriptive, clear, and comprehensively answer the original question. Each paragraph should describe a unique analytic insight dervied from your research. Each analytic insight should be the product of your methods, applied to your data, to provide part of an answer to your research question.

Address any limitations or factors to consider for future research. Make sure to actually answer the question! 

#### 7. Visuals
***13pts***

Provide a graph showing the forecasted counts for each month in 2025. This graph should have a title and correct axis labels (*1pt*).

Provide twelve (yes, 12) hotspot maps. 
at least three visuals of your data. These visuals should be relevant to your research and used as aids to articulate your findings. Think of these as support/evidence for your words. These visuals can be any combination of charts, tables, graphs, maps, or other similar graphics. All visuals should be intuitive and logical, meaning the reader can understand them without an additional explanation. 

- Thoughtful and logical: do they make sense to someone that has never seen them before? (*2pts*)
- All visuals should be somehow, someway referenced in the text (*2pts*) 
- Analytically meaningful: does each visual provide useful, relevant, and actionable insights? (*2pts*) 

#### 8. Formatting
***1pt***

- Error-free writing. No typos, run-on sentences, or sentence fragments. Proper punctuation. Real words. Need help paraphrasing dense content? [Try this](https://quillbot.com/). And [here’s a great reference](https://cognella-titles-sneakpreviews.s3-us-west-2.amazonaws.com/83869-1A-URT/83869-1A_SP.pdf), too.
- File submitted as the knitted HTML output of an RMarkdown file, via Canvas.

ONE LAST THING... the source code for creating this file can be [found here](https://github.com/matthew-danna/criminal-justice-research-methods-data-analysis/blob/main/scripts/2024%20fall/CRIM%20515%20Final%20Project%20Template.Rmd).

Please [email me](mailto:mdanna2@gmu.edu) with any questions.
